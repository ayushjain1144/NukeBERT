{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_pretrained.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayushjain1144/semantic-segmentation-IGCAR/blob/master/bert_pretrained.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVL9-9BKSzSF",
        "colab_type": "code",
        "outputId": "be264b1c-a26d-4424-a88a-91b885477618",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (0.1.82)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpScP8RAWvyU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "6fec44ee-b6b0-4734-bc3b-c81294cedef0"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!ln -s /content/gdrive/My\\ Drive/igcar_ps/ /mydrive"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n",
            "ln: failed to create symbolic link '/mydrive/igcar_ps': File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mexRfQy7Wxvl",
        "colab_type": "code",
        "outputId": "e91e9fa8-c7a1-408c-bbba-11d0c7f87c3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!git clone https://github.com/google-research/bert"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'bert' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ongTGudaW8jY",
        "colab_type": "code",
        "outputId": "1c44bf50-23fe-4c6f-8032-9c7ef04451df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import nltk\n",
        "import random\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "import sentencepiece as spm\n",
        "\n",
        "from glob import glob\n",
        "from google.colab import auth, drive\n",
        "from tensorflow.keras.utils import Progbar\n",
        "\n",
        "sys.path.append(\"bert\")\n",
        "\n",
        "\n",
        "\n",
        "from bert import modeling, optimization, tokenization\n",
        "from bert.run_pretraining import input_fn_builder, model_fn_builder\n",
        "% cd ..\n",
        "log = logging.getLogger('tensorflow')\n",
        "log.setLevel(logging.INFO)\n",
        "\n",
        "formatter = logging.Formatter('%(asctime)s : \\\n",
        "    %(message)s')\n",
        "sh = logging.StreamHandler()\n",
        "sh.setLevel(logging.INFO)\n",
        "sh.setFormatter(formatter)\n",
        "log.handlers = [sh]\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "    log.info(\"Using TPU runtime\")\n",
        "    USE_TPU = True\n",
        "    TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "    \n",
        "    with tf.Session(TPU_ADDRESS) as session:\n",
        "        log.info('TPU address is ' + TPU_ADDRESS)\n",
        "        \n",
        "       \n",
        "else:\n",
        "    log.warning('Not connected to TPU runtime')\n",
        "    USE_TPU = False"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-06-26 09:21:33,093 :     Using TPU runtime\n",
            "I0626 09:21:33.093087 140179299624832 interactiveshell.py:2882] Using TPU runtime\n",
            "2019-06-26 09:21:33,100 :     TPU address is grpc://10.19.255.18:8470\n",
            "I0626 09:21:33.100322 140179299624832 interactiveshell.py:2882] TPU address is grpc://10.19.255.18:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7FefAp_6cKl",
        "colab_type": "code",
        "outputId": "d5acec6f-b689-4e4e-a793-92bb64cd6881",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "%cd /content/\n",
        "!ls"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "1.txt  bert  bert_model  gdrive  pretraining_data  sample_data\tshards\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NKMNUdjR0dqQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# vocabulary formation\n",
        "\n",
        "#MODEL_PREFIX = \"tokenizer\"\n",
        "#VOC_SIZE = 32000\n",
        "#NUM_PLACEHOLDERS = 256\n",
        "#SUBSAMPLE_SIZE = 12800000\n",
        "\n",
        "#SPM_COMMAND = ('--input={} --model_prefix={} '\n",
        "#              '--vocab_size={} --input_sentence_size={} '\n",
        "#              '--shuffle_input_sentence=true '\n",
        "#              '--bos_id=-1 --eos_id=-1').format(\n",
        "#              'out6_pendrive.txt', MODEL_PREFIX,\n",
        "#               VOC_SIZE - NUM_PLACEHOLDERS, SUBSAMPLE_SIZE)\n",
        "#spm.SentencePieceTrainer.Train(SPM_COMMAND)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8MEcIi47rGc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QH_3A5xZ-md",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "from bert import modeling, optimization, tokenization\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuMf4ygEcyLQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import bert.run_pretraining\n",
        "from bert.run_pretraining import input_fn_builder, model_fn_builder\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAu7IdzPc3EW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_SEQ_LENGTH = 128\n",
        "MASKED_LM_PROB = 0.15\n",
        "MAX_PREDICTIONS = 20\n",
        "DO_LOWER_CASE = True\n",
        "PROCESSES = 2\n",
        "PRETRAINING_DIR = \"pretraining_data\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZEwEGutfq-I",
        "colab_type": "code",
        "outputId": "8315afab-d479-46c1-bd9a-df7af51d6cb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!wc -w 1.txt"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "682357 1.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fj1GYeeJfun6",
        "colab_type": "code",
        "outputId": "40aa579b-f9a1-4bc5-8c8d-fc9cbb5d266c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!mkdir ./shards\n",
        "!split -a 4 -l 256000 -d '1.txt' ./shards/shard_"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘./shards’: File exists\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_4xxGjgfwaJ",
        "colab_type": "code",
        "outputId": "1be69192-5b4a-4b8f-9bee-705b528d9a94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls ./shards/"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "shard_0000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kszGK0VCKCqk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "37a3743d-bd0b-4ed4-e4a6-f015e153bbcc"
      },
      "source": [
        "BERT_MODEL = 'uncased_L-12_H-768_A-12'\n",
        "BERT_PRETRAINED_DIR = '/mydrive/bert_uncased/' + BERT_MODEL\n",
        "print('****** BERT pretrained directory: {} *****'.format(BERT_PRETRAINED_DIR)) \n",
        "#!ls BERT_PRETRAINED_DIR"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "****** BERT pretrained directory: /mydrive/bert_uncased/uncased_L-12_H-768_A-12 *****\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWyRoAuZL9X7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BERT_CONFIG = BERT_PRETRAINED_DIR + '/bert_config.json'\n",
        "CHKPT_DIR = BERT_PRETRAINED_DIR + '/bert_model.ckpt'\n",
        "VOCAB_FILE = BERT_PRETRAINED_DIR + '/vocab.txt'\n",
        "INIT_CHECKPOINT = BERT_PRETRAINED_DIR + '/bert_model.ckpt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7A2a1Z2fyzx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "e651bec6-b329-4088-a59b-92318a088859"
      },
      "source": [
        "XARGS_CMD = (\"ls ./shards/ | \"\n",
        "             \"xargs -n 1 -P {} -I{} \"\n",
        "            \"python3 bert/create_pretraining_data.py \"\n",
        "            \"--input_file=./shards/{} \"\n",
        "            \"--output_file={}/{}.tfrecord \"\n",
        "            \"--vocab_file={} \"\n",
        "            \"--do_lower_case={} \"\n",
        "            \"--max_seq_length={} \"\n",
        "            \"--masked_lm_prob={} \"\n",
        "            \"--random_seed=108 \"\n",
        "            \"--dupe_factors=5 \")\n",
        "\n",
        "XARGS_CMD = XARGS_CMD.format(PROCESSES, '{}', '{}',\n",
        "                            PRETRAINING_DIR, '{}',\n",
        "                            VOCAB_FILE,\n",
        "                            DO_LOWER_CASE,\n",
        "                            MAX_PREDICTIONS, MAX_SEQ_LENGTH,\n",
        "                            MASKED_LM_PROB)\n",
        "\n",
        "print(XARGS_CMD)"
      ],
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ls ./shards/ | xargs -n 1 -P 2 -I{} python3 bert/create_pretraining_data.py --input_file=./shards/{} --output_file=pretraining_data/{}.tfrecord --vocab_file=/mydrive/bert_uncased/uncased_L-12_H-768_A-12/vocab.txt --do_lower_case=True --max_seq_length=20 --masked_lm_prob=128 --random_seed=108 --dupe_factors=5 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZMJnUnTRc1h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.gfile.MkDir(PRETRAINING_DIR)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89-dOqD0RoN7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3710
        },
        "outputId": "12aa2481-e8d2-45a1-b23c-ae05f233a579"
      },
      "source": [
        "!$XARGS_CMD"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0626 09:21:55.100078 140447344932736 deprecation_wrapper.py:119] From bert/create_pretraining_data.py:469: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0626 09:21:55.100918 140447344932736 deprecation_wrapper.py:119] From bert/create_pretraining_data.py:437: The name tf.logging.set_verbosity is deprecated. Please use tf.compat.v1.logging.set_verbosity instead.\n",
            "\n",
            "W0626 09:21:55.101079 140447344932736 deprecation_wrapper.py:119] From bert/create_pretraining_data.py:437: The name tf.logging.INFO is deprecated. Please use tf.compat.v1.logging.INFO instead.\n",
            "\n",
            "W0626 09:21:55.101243 140447344932736 deprecation_wrapper.py:119] From /content/bert/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "W0626 09:21:55.224929 140447344932736 deprecation_wrapper.py:119] From bert/create_pretraining_data.py:444: The name tf.gfile.Glob is deprecated. Please use tf.io.gfile.glob instead.\n",
            "\n",
            "W0626 09:21:55.225663 140447344932736 deprecation_wrapper.py:119] From bert/create_pretraining_data.py:446: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0626 09:21:55.225814 140447344932736 create_pretraining_data.py:446] *** Reading from input files ***\n",
            "I0626 09:21:55.225893 140447344932736 create_pretraining_data.py:448]   ./shards/shard_0000\n",
            "I0626 09:23:16.699574 140447344932736 create_pretraining_data.py:457] *** Writing to output files ***\n",
            "I0626 09:23:16.699830 140447344932736 create_pretraining_data.py:459]   pretraining_data/shard_0000.tfrecord\n",
            "W0626 09:23:16.700025 140447344932736 deprecation_wrapper.py:119] From bert/create_pretraining_data.py:101: The name tf.python_io.TFRecordWriter is deprecated. Please use tf.io.TFRecordWriter instead.\n",
            "\n",
            "I0626 09:23:16.720363 140447344932736 create_pretraining_data.py:149] *** Example ***\n",
            "I0626 09:23:16.720601 140447344932736 create_pretraining_data.py:151] tokens: [CLS] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] these [MASK] [SEP] [MASK] [MASK] ##ark [MASK] [MASK] [MASK] which [MASK] [SEP]\n",
            "I0626 09:23:16.720707 140447344932736 create_pretraining_data.py:161] input_ids: 101 103 103 103 103 103 103 103 2122 103 102 103 103 17007 103 103 103 2029 103 102\n",
            "I0626 09:23:16.720792 140447344932736 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0626 09:23:16.720884 140447344932736 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
            "I0626 09:23:16.720968 140447344932736 create_pretraining_data.py:161] masked_lm_positions: 1 2 3 4 5 6 7 8 9 11 12 13 14 15 16 17 18 0 0 0\n",
            "I0626 09:23:16.721060 140447344932736 create_pretraining_data.py:161] masked_lm_ids: 2800 3430 28846 3294 2938 2483 14213 2122 5918 2008 2938 2483 14213 2070 5918 2029 2038 0 0 0\n",
            "I0626 09:23:16.721155 140447344932736 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0\n",
            "I0626 09:23:16.721259 140447344932736 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "I0626 09:23:16.721639 140447344932736 create_pretraining_data.py:149] *** Example ***\n",
            "I0626 09:23:16.721750 140447344932736 create_pretraining_data.py:151] tokens: [CLS] [MASK] [MASK] nod [MASK] eels [MASK] [MASK] preceding [MASK] [SEP] [MASK] [MASK] [MASK] will [MASK] [MASK] [MASK] ##⁰ [SEP]\n",
            "I0626 09:23:16.721853 140447344932736 create_pretraining_data.py:161] input_ids: 101 103 103 7293 103 29317 103 103 11003 103 102 103 103 103 2097 103 103 103 30071 102\n",
            "I0626 09:23:16.721934 140447344932736 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0626 09:23:16.722014 140447344932736 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
            "I0626 09:23:16.722094 140447344932736 create_pretraining_data.py:161] masked_lm_positions: 1 2 3 4 5 6 7 8 9 11 12 13 14 15 16 17 18 0 0 0\n",
            "I0626 09:23:16.722176 140447344932736 create_pretraining_data.py:161] masked_lm_ids: 4504 7065 9355 2256 4824 1996 2824 11003 1996 4500 3257 1998 2097 3921 17816 2029 2794 0 0 0\n",
            "I0626 09:23:16.722282 140447344932736 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0\n",
            "I0626 09:23:16.722355 140447344932736 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0626 09:23:16.722697 140447344932736 create_pretraining_data.py:149] *** Example ***\n",
            "I0626 09:23:16.722798 140447344932736 create_pretraining_data.py:151] tokens: [CLS] [MASK] garrett [MASK] [MASK] and [MASK] [MASK] [MASK] [MASK] [SEP] [MASK] [MASK] [MASK] tube [MASK] [MASK] [MASK] [MASK] [SEP]\n",
            "I0626 09:23:16.722895 140447344932736 create_pretraining_data.py:161] input_ids: 101 103 9674 103 103 1998 103 103 103 103 102 103 103 103 7270 103 103 103 103 102\n",
            "I0626 09:23:16.722975 140447344932736 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0626 09:23:16.723054 140447344932736 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
            "I0626 09:23:16.723135 140447344932736 create_pretraining_data.py:161] masked_lm_positions: 1 2 3 4 5 6 7 8 9 11 12 13 14 15 16 17 18 0 0 0\n",
            "I0626 09:23:16.723364 140447344932736 create_pretraining_data.py:161] masked_lm_ids: 2007 25550 2078 3614 1998 6022 5301 3169 2001 5307 15827 2896 7270 7123 2069 7270 4920 0 0 0\n",
            "I0626 09:23:16.723469 140447344932736 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0\n",
            "I0626 09:23:16.723542 140447344932736 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0626 09:23:16.723881 140447344932736 create_pretraining_data.py:149] *** Example ***\n",
            "I0626 09:23:16.723986 140447344932736 create_pretraining_data.py:151] tokens: [CLS] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] the topping [MASK] [SEP] [MASK] [MASK] [MASK] [MASK] intermediate [MASK] [MASK] ##r [SEP]\n",
            "I0626 09:23:16.724076 140447344932736 create_pretraining_data.py:161] input_ids: 101 103 103 103 103 103 103 1996 22286 103 102 103 103 103 103 7783 103 103 2099 102\n",
            "I0626 09:23:16.724155 140447344932736 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0626 09:23:16.724251 140447344932736 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
            "I0626 09:23:16.724330 140447344932736 create_pretraining_data.py:161] masked_lm_positions: 1 2 3 4 5 6 7 8 9 11 12 13 14 15 16 17 18 0 0 0\n",
            "I0626 09:23:16.724412 140447344932736 create_pretraining_data.py:161] masked_lm_ids: 2240 1996 5492 13103 10388 2007 1996 2004 4168 5100 2053 17644 7264 7783 3684 3863 2099 0 0 0\n",
            "I0626 09:23:16.724498 140447344932736 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0\n",
            "I0626 09:23:16.724568 140447344932736 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0626 09:23:16.724903 140447344932736 create_pretraining_data.py:149] *** Example ***\n",
            "I0626 09:23:16.725002 140447344932736 create_pretraining_data.py:151] tokens: [CLS] change does [MASK] [MASK] betrayal [MASK] [MASK] [MASK] [MASK] [SEP] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] beasts [SEP]\n",
            "I0626 09:23:16.725090 140447344932736 create_pretraining_data.py:161] input_ids: 101 2689 2515 103 103 14583 103 103 103 103 102 103 103 103 103 103 103 103 15109 102\n",
            "I0626 09:23:16.725168 140447344932736 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0626 09:23:16.725264 140447344932736 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
            "I0626 09:23:16.725344 140447344932736 create_pretraining_data.py:161] masked_lm_positions: 1 2 3 4 5 6 7 8 9 11 12 13 14 15 16 17 18 0 0 0\n",
            "I0626 09:23:16.725425 140447344932736 create_pretraining_data.py:161] masked_lm_ids: 2689 2515 2025 5258 1996 2034 3413 2174 3853 14841 2072 14841 2072 13843 14841 2072 14841 0 0 0\n",
            "I0626 09:23:16.725509 140447344932736 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0\n",
            "I0626 09:23:16.725579 140447344932736 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0626 09:23:16.725903 140447344932736 create_pretraining_data.py:149] *** Example ***\n",
            "I0626 09:23:16.726001 140447344932736 create_pretraining_data.py:151] tokens: [CLS] why [MASK] [MASK] [MASK] why [SEP] jointly [MASK] [MASK] [MASK] [MASK] [MASK] wo [MASK] [MASK] [MASK] [MASK] [MASK] [SEP]\n",
            "I0626 09:23:16.726087 140447344932736 create_pretraining_data.py:161] input_ids: 101 2339 103 103 103 2339 102 10776 103 103 103 103 103 24185 103 103 103 103 103 102\n",
            "I0626 09:23:16.726165 140447344932736 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0626 09:23:16.726256 140447344932736 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0626 09:23:16.726342 140447344932736 create_pretraining_data.py:161] masked_lm_positions: 1 2 3 4 5 7 8 9 10 11 12 13 14 15 16 17 18 0 0 0\n",
            "I0626 09:23:16.726421 140447344932736 create_pretraining_data.py:161] masked_lm_ids: 2339 29393 2310 2015 2339 13512 2075 3124 2632 6767 3726 24185 2226 4862 2319 4135 2100 0 0 0\n",
            "I0626 09:23:16.726505 140447344932736 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0\n",
            "I0626 09:23:16.726577 140447344932736 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "I0626 09:23:16.726910 140447344932736 create_pretraining_data.py:149] *** Example ***\n",
            "I0626 09:23:16.727010 140447344932736 create_pretraining_data.py:151] tokens: [CLS] [MASK] tomatoes embrace [MASK] robin [MASK] educational ##vio [MASK] [SEP] [MASK] [MASK] [MASK] [MASK] [MASK] ##fold [MASK] [MASK] [SEP]\n",
            "I0626 09:23:16.727100 140447344932736 create_pretraining_data.py:161] input_ids: 101 103 12851 9979 103 5863 103 4547 25500 103 102 103 103 103 103 103 10371 103 103 102\n",
            "I0626 09:23:16.727179 140447344932736 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0626 09:23:16.727277 140447344932736 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
            "I0626 09:23:16.727358 140447344932736 create_pretraining_data.py:161] masked_lm_positions: 1 2 3 4 5 6 7 8 9 11 12 13 14 15 16 17 18 0 0 0\n",
            "I0626 09:23:16.727440 140447344932736 create_pretraining_data.py:161] masked_lm_ids: 2933 3193 4762 11774 2989 2291 2019 2140 11265 5323 1996 11709 22889 2361 22214 14277 26726 0 0 0\n",
            "I0626 09:23:16.727524 140447344932736 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0\n",
            "I0626 09:23:16.727596 140447344932736 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0626 09:23:16.727925 140447344932736 create_pretraining_data.py:149] *** Example ***\n",
            "I0626 09:23:16.728026 140447344932736 create_pretraining_data.py:151] tokens: [CLS] [MASK] [MASK] [MASK] [MASK] [MASK] the [MASK] [unused955] [MASK] [MASK] the [SEP] [MASK] ##national meek [MASK] [MASK] [MASK] [SEP]\n",
            "I0626 09:23:16.728114 140447344932736 create_pretraining_data.py:161] input_ids: 101 103 103 103 103 103 1996 103 960 103 103 1996 102 103 25434 28997 103 103 103 102\n",
            "I0626 09:23:16.728193 140447344932736 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0626 09:23:16.728288 140447344932736 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1\n",
            "I0626 09:23:16.728370 140447344932736 create_pretraining_data.py:161] masked_lm_positions: 1 2 3 4 5 6 7 8 9 10 11 13 14 15 16 17 18 0 0 0\n",
            "I0626 09:23:16.728450 140447344932736 create_pretraining_data.py:161] masked_lm_ids: 4489 2008 3078 5197 12407 1996 4555 9829 23253 10572 1996 4041 1998 9312 2622 4187 7885 0 0 0\n",
            "I0626 09:23:16.728540 140447344932736 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0\n",
            "I0626 09:23:16.728624 140447344932736 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0626 09:23:16.728974 140447344932736 create_pretraining_data.py:149] *** Example ***\n",
            "I0626 09:23:16.729079 140447344932736 create_pretraining_data.py:151] tokens: [CLS] the [MASK] [MASK] int [MASK] [MASK] [SEP] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [SEP]\n",
            "I0626 09:23:16.729169 140447344932736 create_pretraining_data.py:161] input_ids: 101 1996 103 103 20014 103 103 102 103 103 103 103 103 103 103 103 103 103 103 102\n",
            "I0626 09:23:16.729262 140447344932736 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0626 09:23:16.729342 140447344932736 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0626 09:23:16.729424 140447344932736 create_pretraining_data.py:161] masked_lm_positions: 1 2 3 4 5 6 8 9 10 11 12 13 14 15 16 17 18 0 0 0\n",
            "I0626 09:23:16.729504 140447344932736 create_pretraining_data.py:161] masked_lm_ids: 1996 12612 2719 20014 5369 5783 2071 2025 4453 5107 10569 2021 2001 4453 13091 13722 2005 0 0 0\n",
            "I0626 09:23:16.729588 140447344932736 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0\n",
            "I0626 09:23:16.729659 140447344932736 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "I0626 09:23:16.729985 140447344932736 create_pretraining_data.py:149] *** Example ***\n",
            "I0626 09:23:16.730088 140447344932736 create_pretraining_data.py:151] tokens: [CLS] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] ##s [MASK] [MASK] [MASK] [MASK] thumbs [MASK] [SEP] kelley [MASK] mines risky [SEP]\n",
            "I0626 09:23:16.730176 140447344932736 create_pretraining_data.py:161] input_ids: 101 103 103 103 103 103 103 2015 103 103 103 103 16784 103 102 19543 103 7134 19188 102\n",
            "I0626 09:23:16.730271 140447344932736 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0626 09:23:16.730350 140447344932736 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1\n",
            "I0626 09:23:16.730431 140447344932736 create_pretraining_data.py:161] masked_lm_positions: 1 2 3 4 5 6 7 8 9 10 11 12 13 15 16 17 18 0 0 0\n",
            "I0626 09:23:16.730512 140447344932736 create_pretraining_data.py:161] masked_lm_ids: 2256 4824 1996 5512 2389 17978 2015 2008 4503 1996 17864 28688 2076 19543 4879 7134 3189 0 0 0\n",
            "I0626 09:23:16.730597 140447344932736 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0\n",
            "I0626 09:23:16.730667 140447344932736 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0626 09:23:16.731049 140447344932736 create_pretraining_data.py:149] *** Example ***\n",
            "I0626 09:23:16.731161 140447344932736 create_pretraining_data.py:151] tokens: [CLS] [MASK] second [MASK] [MASK] ##down [MASK] [MASK] ru [MASK] [SEP] outrageous [MASK] [MASK] [MASK] ##g [MASK] [MASK] temper [SEP]\n",
            "I0626 09:23:16.731265 140447344932736 create_pretraining_data.py:161] input_ids: 101 103 2117 103 103 7698 103 103 21766 103 102 25506 103 103 103 2290 103 103 12178 102\n",
            "I0626 09:23:16.731346 140447344932736 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0626 09:23:16.731426 140447344932736 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
            "I0626 09:23:16.731507 140447344932736 create_pretraining_data.py:161] masked_lm_positions: 1 2 3 4 5 6 7 8 9 11 12 13 14 15 16 17 18 0 0 0\n",
            "I0626 09:23:16.731587 140447344932736 create_pretraining_data.py:161] masked_lm_ids: 19485 24964 3366 2907 7698 5080 2391 2073 2035 4313 21456 28313 12098 2290 2445 1996 12178 0 0 0\n",
            "I0626 09:23:16.731672 140447344932736 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0\n",
            "I0626 09:23:16.731743 140447344932736 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0626 09:23:16.732072 140447344932736 create_pretraining_data.py:149] *** Example ***\n",
            "I0626 09:23:16.732175 140447344932736 create_pretraining_data.py:151] tokens: [CLS] [MASK] [MASK] accreditation [SEP] [MASK] [MASK] [MASK] [MASK] ee ##oes [MASK] [MASK] [MASK] insights [MASK] [MASK] [MASK] [MASK] [SEP]\n",
            "I0626 09:23:16.732280 140447344932736 create_pretraining_data.py:161] input_ids: 101 103 103 14893 102 103 103 103 103 25212 22504 103 103 103 20062 103 103 103 103 102\n",
            "I0626 09:23:16.732361 140447344932736 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0626 09:23:16.732439 140447344932736 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0626 09:23:16.732519 140447344932736 create_pretraining_data.py:161] masked_lm_positions: 1 2 3 5 6 7 8 9 10 11 12 13 14 15 16 17 18 0 0 0\n",
            "I0626 09:23:16.732599 140447344932736 create_pretraining_data.py:161] masked_lm_ids: 2812 7504 3091 4654 2361 8292 2063 25212 22504 3806 17170 12495 18719 24872 21392 2015 2522 0 0 0\n",
            "I0626 09:23:16.732682 140447344932736 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0\n",
            "I0626 09:23:16.732754 140447344932736 create_pretraining_data.py:161] next_sentence_labels: 0\n",
            "I0626 09:23:16.733078 140447344932736 create_pretraining_data.py:149] *** Example ***\n",
            "I0626 09:23:16.733175 140447344932736 create_pretraining_data.py:151] tokens: [CLS] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] utility [MASK] [MASK] ##ne [MASK] [MASK] [MASK] [SEP] [MASK] [MASK] [SEP]\n",
            "I0626 09:23:16.733277 140447344932736 create_pretraining_data.py:161] input_ids: 101 103 103 103 103 103 103 103 103 9710 103 103 2638 103 103 103 102 103 103 102\n",
            "I0626 09:23:16.733356 140447344932736 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0626 09:23:16.733436 140447344932736 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1\n",
            "I0626 09:23:16.733516 140447344932736 create_pretraining_data.py:161] masked_lm_positions: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 17 18 0 0 0\n",
            "I0626 09:23:16.733596 140447344932736 create_pretraining_data.py:161] masked_lm_ids: 16222 12260 21716 15141 9099 8566 17119 2019 2140 12098 7446 2638 2120 5911 2640 2474 2050 0 0 0\n",
            "I0626 09:23:16.733680 140447344932736 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0\n",
            "I0626 09:23:16.733751 140447344932736 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0626 09:23:16.734073 140447344932736 create_pretraining_data.py:149] *** Example ***\n",
            "I0626 09:23:16.734169 140447344932736 create_pretraining_data.py:151] tokens: [CLS] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] ee [MASK] [SEP] [MASK] [MASK] sm [MASK] [MASK] [MASK] [MASK] [MASK] [SEP]\n",
            "I0626 09:23:16.734271 140447344932736 create_pretraining_data.py:161] input_ids: 101 103 103 103 103 103 103 103 25212 103 102 103 103 15488 103 103 103 103 103 102\n",
            "I0626 09:23:16.734352 140447344932736 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0626 09:23:16.734430 140447344932736 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
            "I0626 09:23:16.806403 140447344932736 create_pretraining_data.py:161] masked_lm_positions: 1 2 3 4 5 6 7 8 9 11 12 13 14 15 16 17 18 0 0 0\n",
            "I0626 09:23:16.806653 140447344932736 create_pretraining_data.py:161] masked_lm_ids: 2063 9413 2063 4372 2063 9413 2078 25212 7869 3501 2094 15488 2063 28816 2063 21358 2213 0 0 0\n",
            "I0626 09:23:16.806795 140447344932736 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0\n",
            "I0626 09:23:16.806922 140447344932736 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0626 09:23:16.807583 140447344932736 create_pretraining_data.py:149] *** Example ***\n",
            "I0626 09:23:16.807789 140447344932736 create_pretraining_data.py:151] tokens: [CLS] [MASK] [MASK] [MASK] capcom instrument [MASK] [MASK] [MASK] [MASK] [SEP] [MASK] [MASK] [MASK] ##ф [MASK] [MASK] [MASK] [MASK] [SEP]\n",
            "I0626 09:23:16.807956 140447344932736 create_pretraining_data.py:161] input_ids: 101 103 103 103 26861 6602 103 103 103 103 102 103 103 103 29749 103 103 103 103 102\n",
            "I0626 09:23:16.808085 140447344932736 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0626 09:23:16.808231 140447344932736 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
            "I0626 09:23:16.808359 140447344932736 create_pretraining_data.py:161] masked_lm_positions: 1 2 3 4 5 6 7 8 9 11 12 13 14 15 16 17 18 0 0 0\n",
            "I0626 09:23:16.808483 140447344932736 create_pretraining_data.py:161] masked_lm_ids: 15070 1996 2034 2048 6602 2098 19815 18269 2015 2060 3558 8474 7591 4860 1998 29170 5733 0 0 0\n",
            "I0626 09:23:16.808611 140447344932736 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0\n",
            "I0626 09:23:16.808725 140447344932736 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0626 09:23:16.809338 140447344932736 create_pretraining_data.py:149] *** Example ***\n",
            "I0626 09:23:16.809513 140447344932736 create_pretraining_data.py:151] tokens: [CLS] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [SEP] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] binding ض [SEP]\n",
            "I0626 09:23:16.809654 140447344932736 create_pretraining_data.py:161] input_ids: 101 103 103 103 103 103 103 103 103 103 102 103 103 103 103 103 103 8031 1285 102\n",
            "I0626 09:23:16.809752 140447344932736 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0626 09:23:16.809854 140447344932736 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
            "I0626 09:23:16.809945 140447344932736 create_pretraining_data.py:161] masked_lm_positions: 1 2 3 4 5 6 7 8 9 11 12 13 14 15 16 17 18 0 0 0\n",
            "I0626 09:23:16.810036 140447344932736 create_pretraining_data.py:161] masked_lm_ids: 10536 22196 6914 2291 16014 1996 4353 2122 3787 2640 2515 2025 2421 2030 18513 2075 1996 0 0 0\n",
            "I0626 09:23:16.810131 140447344932736 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0\n",
            "I0626 09:23:16.810247 140447344932736 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0626 09:23:16.810644 140447344932736 create_pretraining_data.py:149] *** Example ***\n",
            "I0626 09:23:16.810758 140447344932736 create_pretraining_data.py:151] tokens: [CLS] [MASK] [MASK] ##gs [MASK] [MASK] [MASK] [MASK] [MASK] ##lan [SEP] thud [MASK] [MASK] ##sy [MASK] cromwell [MASK] [MASK] [SEP]\n",
            "I0626 09:23:16.810880 140447344932736 create_pretraining_data.py:161] input_ids: 101 103 103 5620 103 103 103 103 103 5802 102 20605 103 103 6508 103 16759 103 103 102\n",
            "I0626 09:23:16.810973 140447344932736 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0626 09:23:16.811062 140447344932736 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
            "I0626 09:23:16.811150 140447344932736 create_pretraining_data.py:161] masked_lm_positions: 1 2 3 4 5 6 7 8 9 11 12 13 14 15 16 17 18 0 0 0\n",
            "I0626 09:23:16.811258 140447344932736 create_pretraining_data.py:161] masked_lm_ids: 5498 12734 5620 14289 8737 2368 4315 26927 14503 2094 10695 2061 6508 2906 4523 18752 28745 0 0 0\n",
            "I0626 09:23:16.811356 140447344932736 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0\n",
            "I0626 09:23:16.811442 140447344932736 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0626 09:23:16.811811 140447344932736 create_pretraining_data.py:149] *** Example ***\n",
            "I0626 09:23:16.811928 140447344932736 create_pretraining_data.py:151] tokens: [CLS] [MASK] [MASK] [MASK] urgency [MASK] [MASK] [MASK] [MASK] [MASK] [SEP] [MASK] brownish [MASK] lenses [MASK] [MASK] [MASK] [MASK] [SEP]\n",
            "I0626 09:23:16.812028 140447344932736 create_pretraining_data.py:161] input_ids: 101 103 103 103 19353 103 103 103 103 103 102 103 19437 103 15072 103 103 103 103 102\n",
            "I0626 09:23:16.812118 140447344932736 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0626 09:23:16.812224 140447344932736 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
            "I0626 09:23:16.812312 140447344932736 create_pretraining_data.py:161] masked_lm_positions: 1 2 3 4 5 6 7 8 9 11 12 13 14 15 16 17 18 0 0 0\n",
            "I0626 09:23:16.812404 140447344932736 create_pretraining_data.py:161] masked_lm_ids: 21877 22592 2181 2348 1996 10479 10903 2001 1998 6822 8534 3149 2892 12314 20167 2015 2109 0 0 0\n",
            "I0626 09:23:16.812497 140447344932736 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0\n",
            "I0626 09:23:16.812582 140447344932736 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0626 09:23:16.812952 140447344932736 create_pretraining_data.py:149] *** Example ***\n",
            "I0626 09:23:16.813065 140447344932736 create_pretraining_data.py:151] tokens: [CLS] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] ##te musique [SEP] [MASK] emi [MASK] [MASK] [MASK] [MASK] [MASK] ##א [SEP]\n",
            "I0626 09:23:16.813168 140447344932736 create_pretraining_data.py:161] input_ids: 101 103 103 103 103 103 103 103 2618 25784 102 103 12495 103 103 103 103 103 29788 102\n",
            "I0626 09:23:16.813273 140447344932736 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0626 09:23:16.813361 140447344932736 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
            "I0626 09:23:16.813452 140447344932736 create_pretraining_data.py:161] masked_lm_positions: 1 2 3 4 5 6 7 8 9 11 12 13 14 15 16 17 18 0 0 0\n",
            "I0626 09:23:16.813542 140447344932736 create_pretraining_data.py:161] masked_lm_ids: 4143 7317 3854 8386 2368 12849 10695 2618 27617 25354 4860 5806 2005 13365 2689 4860 10819 0 0 0\n",
            "I0626 09:23:16.813636 140447344932736 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0\n",
            "I0626 09:23:16.813714 140447344932736 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0626 09:23:16.814095 140447344932736 create_pretraining_data.py:149] *** Example ***\n",
            "I0626 09:23:16.814220 140447344932736 create_pretraining_data.py:151] tokens: [CLS] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [SEP] ##lves [MASK] [MASK] [MASK] [MASK] pcs [MASK] [MASK] [SEP]\n",
            "I0626 09:23:16.814322 140447344932736 create_pretraining_data.py:161] input_ids: 101 103 103 103 103 103 103 103 103 103 102 20899 103 103 103 103 27019 103 103 102\n",
            "I0626 09:23:16.814414 140447344932736 create_pretraining_data.py:161] input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            "I0626 09:23:16.814502 140447344932736 create_pretraining_data.py:161] segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1\n",
            "I0626 09:23:16.814591 140447344932736 create_pretraining_data.py:161] masked_lm_positions: 1 2 3 4 5 6 7 8 9 11 12 13 14 15 16 17 18 0 0 0\n",
            "I0626 09:23:16.814681 140447344932736 create_pretraining_data.py:161] masked_lm_ids: 3988 3785 2024 3205 2795 3523 2005 1996 2698 6814 2008 2682 3464 3565 16846 4648 3064 0 0 0\n",
            "I0626 09:23:16.814773 140447344932736 create_pretraining_data.py:161] masked_lm_weights: 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 1.0 0.0 0.0 0.0\n",
            "I0626 09:23:16.814862 140447344932736 create_pretraining_data.py:161] next_sentence_labels: 1\n",
            "I0626 09:24:23.486552 140447344932736 create_pretraining_data.py:166] Wrote 338585 total instances\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qh-umioj72xO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAwgehFAEEbw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MODEL_DIR = \"bert_model\"\n",
        "tf.gfile.MkDir(MODEL_DIR)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28sBjWx_Rrxx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hyperparameters for BERT BASE\n",
        "\n",
        "bert_base_config = {\n",
        "    \"attention_probs_dropout_prob\": 0.1,\n",
        "    \"directionality\": \"bidi\",\n",
        "    \"hidden_act\": \"gelu\",\n",
        "    \"hidden_dropout_prob\": 0.1,\n",
        "    \"hidden_size\": 768,\n",
        "    \"initializer_range\": 0.02,\n",
        "    \"intermediate_size\": 3072,\n",
        "    \"max_position_embeddings\": 512,\n",
        "    \"num_attention_heads\": 12,\n",
        "    \"num_hidden_layers\": 12,\n",
        "    \"pooler_fc_size\": 768,\n",
        "    \"pooler_num_attention_heads\": 12,\n",
        "    \"pooler_num_fc_layers\": 3,\n",
        "    \"pooler_size_per_head\": 128,\n",
        "    \"pooler_type\": \"first_token_transform\",\n",
        "    \"vocab_size\": 30522\n",
        "}\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX5ci_dk9g9e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "with open(\"{}/bert_config.json\".format(MODEL_DIR), \"w\") as base:\n",
        "    json.dump(bert_base_config, base, indent=2)\n",
        "    \n",
        "with open(\"{}/bert_vocab\".format(MODEL_DIR), \"w\") as vocab:\n",
        "    vocab_bert = open(VOCAB_FILE, 'r').read()\n",
        "    vocab.write(vocab_bert)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbzY0cjyL27Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r $MODEL_DIR $PRETRAINING_DIR /mydrive/"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JgZ3QCkILhPj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TO DO\n",
        "#code to transfer latest checkpoint from another directory to new directory\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_7D763pTxup",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "78f21727-b71f-473c-d3d8-c91542acc372"
      },
      "source": [
        "TRAIN_BATCH_SIZE = 128\n",
        "MAX_PREDICTIONS =20\n",
        "MAX_SEQ_LENGTH = 128\n",
        "MASKED_LM_PROB = 0.15\n",
        "\n",
        "EVAL_BATCH_SIZE = 64\n",
        "LEARNING_RATE = 2e-5\n",
        "TRAIN_STEPS = 1000\n",
        "SAVE_CHECKPOINTS_STEPS = 100\n",
        "NUM_TPU_CORES = 8\n",
        "\n",
        "BERT_DRIVE_DIR = \"{}/{}\".format('/mydrive', MODEL_DIR)\n",
        "DATA_DRIVE_DIR = \"{}/{}\".format('/mydrive', PRETRAINING_DIR)\n",
        "\n",
        "PATH_TO_CHECKPOINT = os.path.join(BERT_DRIVE_DIR, \"bert_model.ckpt\")\n",
        "\n",
        "INIT_CHECKPOINT = tf.train.latest_checkpoint(PATH_TO_CHECKPOINT)\n",
        "\n",
        "if INIT_CHECKPOINT == None:\n",
        "    print(\"no checkpoint found, loading the default\")\n",
        "    INIT_CHECKPOINT = tf.train.load_checkpoint(PATH_TO_CHECKPOINT)\n",
        "\n",
        "CONFIG_FILE = os.path.join(BERT_DRIVE_DIR, \"bert_config.json\")\n",
        "VOCAB_FILE = os.path.join(BERT_DRIVE_DIR, \"bert_vocab\")\n",
        "\n",
        "bert_config = modeling.BertConfig.from_json_file(CONFIG_FILE)\n",
        "input_files = tf.gfile.Glob(os.path.join(DATA_DRIVE_DIR, '*tfrecord'))\n",
        "\n",
        "log.info(\"Using checkpoint: {}\".format(INIT_CHECKPOINT))\n",
        "log.info(\"Using {} data shards\".format(len(input_files)))"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2019-06-26 11:29:31,331 :     Using checkpoint: <tensorflow.python.pywrap_tensorflow_internal.CheckpointReader; proxy of <Swig Object of type 'tensorflow::checkpoint::CheckpointReader *' at 0x7f7dba2c1a20> >\n",
            "I0626 11:29:31.331611 140179299624832 interactiveshell.py:2882] Using checkpoint: <tensorflow.python.pywrap_tensorflow_internal.CheckpointReader; proxy of <Swig Object of type 'tensorflow::checkpoint::CheckpointReader *' at 0x7f7dba2c1a20> >\n",
            "2019-06-26 11:29:31,335 :     Using 1 data shards\n",
            "I0626 11:29:31.335389 140179299624832 interactiveshell.py:2882] Using 1 data shards\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "no checkpoint found, loading the default\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8CRUnlpV09M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_fn = model_fn_builder(\n",
        "    bert_config = bert_config,\n",
        "    init_checkpoint=INIT_CHECKPOINT,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_train_steps=TRAIN_STEPS,\n",
        "    num_warmup_steps=10,\n",
        "    use_tpu=USE_TPU,\n",
        "    use_one_hot_embeddings=True)\n",
        "\n",
        "\n",
        "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "\n",
        "run_config = tf.contrib.tpu.RunConfig(\n",
        "            cluster=tpu_cluster_resolver,\n",
        "            model_dir=BERT_DRIVE_DIR,\n",
        "            save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "            tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "            iterations_per_loop=SAVE_CHECKPOINTS_STEPS,\n",
        "            num_shards=NUM_TPU_CORES,\n",
        "            per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "\n",
        "estimator = tf.contrib.tpu.TPUEstimator(\n",
        "    use_tpu=USE_TPU,\n",
        "    model_fn=model_fn,\n",
        "    config=run_config,\n",
        "    train_batch_size=TRAIN_BATCH_SIZE,\n",
        "    eval_batch_size=EVAL_BATCH_SIZE)\n",
        "\n",
        "train_input_fn = input_fn_builder(\n",
        "    input_files=input_files,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    max_predictions_per_seq=MAX_PREDICTIONS,\n",
        "    is_training=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOdgIx2aIcCa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "estimator.train(input_fn=train_input_fn, max_steps = TRAIN_STEPS )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h07cHEIHZp57",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "5534a4cb-4be7-4f5b-bfbd-1761c5e4da4f"
      },
      "source": [
        "!python3 bert/run_pretraining.py \\\n",
        "--input_file=$input_files \\\n",
        "--output_dir=$BERT_DRIVE_DIR \\\n",
        "--do_train=True \\\n",
        "--do_eval=True \\\n",
        "--bert_config_file=$CONFIG_FILE \\\n",
        "--init_checkpoint=$INIT_CHECKPOINT \\\n",
        "--train_batch_size=128 \\\n",
        "--max_seq_length=128 \\\n",
        "--num_train_steps=1000 \\\n",
        "--num_warmup_steps=10 \\\n",
        "--learning_rate=2e-5 \\\n",
        "--use_tpu=True"
      ],
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: -c: line 0: syntax error near unexpected token `>'\n",
            "/bin/bash: -c: line 0: `python3 bert/run_pretraining.py --input_file=['/mydrive/pretraining_data/shard_0000.tfrecord'] --output_dir=/mydrive/bert_model --do_train=True --do_eval=True --bert_config_file=/mydrive/bert_model/bert_config.json --init_checkpoint=<tensorflow.python.pywrap_tensorflow_internal.CheckpointReader; proxy of <Swig Object of type 'tensorflow::checkpoint::CheckpointReader *' at 0x7f7dba2c1a20> > --train_batch_size=128 --max_seq_length=128 --num_train_steps=1000 --num_warmup_steps=10 --learning_rate=2e-5 --use_tpu=True'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZj4zT6qiyl0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}