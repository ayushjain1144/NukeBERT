{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "bert_pretrained.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qpScP8RAWvyU"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "!ln -s /content/gdrive/My\\ Drive/igcar_ps/ /mydrive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mexRfQy7Wxvl"
      },
      "source": [
        "\n",
        "!git clone https://github.com/google-research/bert"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ongTGudaW8jY"
      },
      "source": [
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "import logging\n",
        "import tensorflow as tf\n",
        "\n",
        "from glob import glob\n",
        "from google.colab import auth, drive\n",
        "from tensorflow.keras.utils import Progbar\n",
        "\n",
        "sys.path.append(\"bert\")\n",
        "\n",
        "\n",
        "\n",
        "from bert import modeling, optimization, tokenization\n",
        "from bert.run_pretraining import input_fn_builder, model_fn_builder\n",
        "\n",
        "auth.authenticate_user()\n",
        "\n",
        "log = logging.getLogger('tensorflow')\n",
        "log.setLevel(logging.INFO)\n",
        "\n",
        "formatter = logging.Formatter('%(asctime)s : \\\n",
        "    %(message)s')\n",
        "sh = logging.StreamHandler()\n",
        "sh.setLevel(logging.INFO)\n",
        "sh.setFormatter(formatter)\n",
        "log.handlers = [sh]\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "    log.info(\"Using TPU runtime\")\n",
        "    USE_TPU = True\n",
        "    TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "    \n",
        "    with tf.Session(TPU_ADDRESS) as session:\n",
        "        log.info('TPU address is ' + TPU_ADDRESS)\n",
        "        with open('/content/adc.json', 'r') as f:\n",
        "          auth_info = json.load(f)\n",
        "        \n",
        "        tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "        \n",
        "       \n",
        "else:\n",
        "    log.warning('Not connected to TPU runtime')\n",
        "    USE_TPU = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7FefAp_6cKl"
      },
      "source": [
        "%cd /content/\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o8MEcIi47rGc"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QH_3A5xZ-md"
      },
      "source": [
        "\n",
        "from bert import modeling, optimization, tokenization\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CuMf4ygEcyLQ"
      },
      "source": [
        "import bert.run_pretraining\n",
        "from bert.run_pretraining import input_fn_builder, model_fn_builder\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAu7IdzPc3EW"
      },
      "source": [
        "MAX_SEQ_LENGTH = 128\n",
        "MASKED_LM_PROB = 0.15\n",
        "MAX_PREDICTIONS = 20\n",
        "DO_LOWER_CASE = True\n",
        "PROCESSES = 2\n",
        "PRETRAINING_DIR = \"pretraining_data\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZEwEGutfq-I"
      },
      "source": [
        "!wc -w final_data_newer.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fj1GYeeJfun6"
      },
      "source": [
        "!mkdir ./shards\n",
        "!split -a 4 -l 256000 -d 'final_data_newer.txt' ./shards/shard_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_4xxGjgfwaJ"
      },
      "source": [
        "!ls ./shards/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kszGK0VCKCqk"
      },
      "source": [
        "BERT_MODEL = 'uncased_L-12_H-768_A-12'\n",
        "BERT_PRETRAINED_DIR = '/mydrive/bert_uncased/' + BERT_MODEL\n",
        "print('****** BERT pretrained directory: {} *****'.format(BERT_PRETRAINED_DIR)) \n",
        "#!ls BERT_PRETRAINED_DIR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWyRoAuZL9X7"
      },
      "source": [
        "BERT_CONFIG = BERT_PRETRAINED_DIR + '/bert_config.json'\n",
        "CHKPT_DIR = BERT_PRETRAINED_DIR + '/bert_model.ckpt.*'\n",
        "VOCAB_FILE = BERT_PRETRAINED_DIR + '/vocab.txt'\n",
        "INIT_CHECKPOINT = BERT_PRETRAINED_DIR + '/bert_model.ckpt'\n",
        "!ls $CHKPT_DIR"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7A2a1Z2fyzx"
      },
      "source": [
        "XARGS_CMD = (\"ls ./shards/ | \"\n",
        "             \"xargs -n 1 -P {} -I{} \"\n",
        "            \"python3 bert/create_pretraining_data.py \"\n",
        "            \"--input_file=./shards/{} \"\n",
        "            \"--output_file={}/{}.tfrecord \"\n",
        "            \"--vocab_file={} \"\n",
        "            \"--do_lower_case={} \"\n",
        "            \"--max_predictions_per_seq={} \"\n",
        "            \"--max_seq_length={} \"\n",
        "            \"--masked_lm_prob={} \"\n",
        "            \"--random_seed=108 \"\n",
        "            \"--dupe_factors=5 \")\n",
        "\n",
        "XARGS_CMD = XARGS_CMD.format(PROCESSES, '{}', '{}',\n",
        "                            PRETRAINING_DIR, '{}',\n",
        "                            VOCAB_FILE,\n",
        "                            DO_LOWER_CASE,\n",
        "                            MAX_PREDICTIONS, MAX_SEQ_LENGTH,\n",
        "                            MASKED_LM_PROB)\n",
        "\n",
        "print(XARGS_CMD)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZMJnUnTRc1h"
      },
      "source": [
        "tf.gfile.MkDir(PRETRAINING_DIR)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89-dOqD0RoN7"
      },
      "source": [
        "!$XARGS_CMD"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qh-umioj72xO"
      },
      "source": [
        "\n",
        "BUCKET_NAME = \"ayushjain1144-bucket\"\n",
        "MODEL_DIR = \"bert_model\"\n",
        "\n",
        "if not BUCKET_NAME:\n",
        "  log.warning(\"Warning: no bucket\")\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAwgehFAEEbw"
      },
      "source": [
        "MODEL_DIR = \"bert_model\"\n",
        "tf.gfile.MkDir(MODEL_DIR)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "28sBjWx_Rrxx"
      },
      "source": [
        "\n",
        "# hyperparameters for BERT BASE\n",
        "\n",
        "bert_base_config = {\n",
        "    \"attention_probs_dropout_prob\": 0.1,\n",
        "    \"directionality\": \"bidi\",\n",
        "    \"hidden_act\": \"gelu\",\n",
        "    \"hidden_dropout_prob\": 0.1,\n",
        "    \"hidden_size\": 768,\n",
        "    \"initializer_range\": 0.02,\n",
        "    \"intermediate_size\": 3072,\n",
        "    \"max_position_embeddings\": 512,\n",
        "    \"num_attention_heads\": 12,\n",
        "    \"num_hidden_layers\": 12,\n",
        "    \"pooler_fc_size\": 768,\n",
        "    \"pooler_num_attention_heads\": 12,\n",
        "    \"pooler_num_fc_layers\": 3,\n",
        "    \"pooler_size_per_head\": 128,\n",
        "    \"pooler_type\": \"first_token_transform\",\n",
        "    \"vocab_size\": 30522\n",
        "}\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IX5ci_dk9g9e"
      },
      "source": [
        "\n",
        "!cp $BERT_CONFIG $MODEL_DIR/\n",
        "!cp $CHKPT_DIR $MODEL_DIR/\n",
        "!ls $MODEL_DIR/\n",
        "    \n",
        "with open(\"{}/bert_vocab.txt\".format(MODEL_DIR), \"w\") as vocab:\n",
        "    vocab_bert = open(VOCAB_FILE, 'r').read()\n",
        "    vocab.write(vocab_bert)\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbzY0cjyL27Q"
      },
      "source": [
        "!cp -r $MODEL_DIR $PRETRAINING_DIR /mydrive/\n",
        "\n",
        "if BUCKET_NAME:\n",
        "  !gsutil -m cp -r $MODEL_DIR $PRETRAINING_DIR gs://$BUCKET_NAME/original/\n",
        "else:\n",
        "  print(\"Not able to copy\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ChKoqD9b88S"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z_7D763pTxup"
      },
      "source": [
        "TRAIN_BATCH_SIZE = 128\n",
        "MAX_PREDICTIONS =20\n",
        "MAX_SEQ_LENGTH = 128\n",
        "MASKED_LM_PROB = 0.15\n",
        "\n",
        "EVAL_BATCH_SIZE = 64\n",
        "LEARNING_RATE = 2e-5\n",
        "TRAIN_STEPS = 40000\n",
        "SAVE_CHECKPOINTS_STEPS = 5000\n",
        "NUM_TPU_CORES = 8\n",
        "\n",
        "BERT_DRIVE_DIR = \"{}/{}\".format('/mydrive', MODEL_DIR)\n",
        "DATA_DRIVE_DIR = \"{}/{}\".format('/mydrive', PRETRAINING_DIR)\n",
        "\n",
        "if BUCKET_NAME:\n",
        "  BUCKET_PATH = \"gs://{}/original\".format(BUCKET_NAME)\n",
        "else:\n",
        "  print(\"bucket name not found\")\n",
        "\n",
        "BERT_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, MODEL_DIR)\n",
        "DATA_GCS_DIR = \"{}/{}\".format(BUCKET_PATH, PRETRAINING_DIR)\n",
        "\n",
        "PATH_TO_CHECKPOINT = os.path.join(BERT_GCS_DIR, \"bert_model.ckpt\")\n",
        "\n",
        "INIT_CHECKPOINT = tf.train.latest_checkpoint(BERT_GCS_DIR)\n",
        "\n",
        "\n",
        "if INIT_CHECKPOINT == None:\n",
        "    print(\"no checkpoint found, loading the default\")\n",
        "    INIT_CHECKPOINT = PATH_TO_CHECKPOINT\n",
        "\n",
        "\n",
        "\n",
        "CONFIG_FILE = os.path.join(BERT_GCS_DIR, \"bert_config.json\")\n",
        "VOCAB_FILE = os.path.join(BERT_GCS_DIR, \"bert_vocab.txt\")\n",
        "\n",
        "bert_config = modeling.BertConfig.from_json_file(CONFIG_FILE)\n",
        "input_files = tf.gfile.Glob(os.path.join(DATA_GCS_DIR, '*tfrecord'))\n",
        "\n",
        "log.info(\"Using checkpoint: {}\".format(INIT_CHECKPOINT))\n",
        "log.info(\"Using {} data shards\".format(len(input_files)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8CRUnlpV09M"
      },
      "source": [
        "model_fn = model_fn_builder(\n",
        "    bert_config = bert_config,\n",
        "    init_checkpoint= INIT_CHECKPOINT,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_train_steps=TRAIN_STEPS,\n",
        "    num_warmup_steps=4000,\n",
        "    use_tpu=True,\n",
        "    use_one_hot_embeddings=True)\n",
        "\n",
        "\n",
        "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "\n",
        "run_config = tf.contrib.tpu.RunConfig(\n",
        "            cluster=tpu_cluster_resolver,\n",
        "            model_dir=BERT_GCS_DIR,\n",
        "            save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "            tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "            iterations_per_loop=SAVE_CHECKPOINTS_STEPS,\n",
        "            num_shards=NUM_TPU_CORES,\n",
        "            per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "\n",
        "estimator = tf.contrib.tpu.TPUEstimator(\n",
        "    use_tpu=True,\n",
        "    model_fn=model_fn,\n",
        "    config=run_config,\n",
        "    train_batch_size=TRAIN_BATCH_SIZE,\n",
        "    eval_batch_size=EVAL_BATCH_SIZE)\n",
        "\n",
        "train_input_fn = input_fn_builder(\n",
        "    input_files=input_files,\n",
        "    max_seq_length=MAX_SEQ_LENGTH,\n",
        "    max_predictions_per_seq=MAX_PREDICTIONS,\n",
        "    is_training=True)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jOdgIx2aIcCa"
      },
      "source": [
        "\n",
        "estimator.train(input_fn=train_input_fn, max_steps = TRAIN_STEPS )\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}